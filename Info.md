Great! I'll prepare a detailed overview of fine-tuning methods, including their historical development, key methodologies, and an in-depth explanation of the most relevant techniques today. I’ll also compare their computational costs and real-world applications, and include PyTorch-based code examples to illustrate their implementation.

I'll let you know once the research is ready.

# Fine-Tuning Methods: A Comprehensive Overview

## 1. History and Evolution of Fine-Tuning

Early concepts of *fine-tuning* in machine learning trace back to the idea of **transfer learning**, where knowledge from one task or dataset is used to help train a model on another task. In the pre-deep learning era, this often meant using features learned by one model as inputs to another, or **pre-training** part of a model and then adapting it to a new task. For example, multi-layer neural networks in the 1990s could be pre-trained on an auxiliary task or in an unsupervised manner and then fine-tuned with supervised learning for a target task. An influential early success was the use of **unsupervised pre-training** by Hinton et al. (2006), who trained deep belief networks layer-by-layer without labels and then fine-tuned the whole network with labeled data, achieving better classification accuracy on MNIST digits than training from scratch ([](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf#:~:text=learning%20procedure%20that%20fine,energy%20landscape%20of%20the)). This demonstrated that *pre-training + fine-tuning* could overcome optimization difficulties in deep networks.

As datasets grew and models became deeper, **fine-tuning gained prominence in deep learning**. A common practice emerged in computer vision after the success of AlexNet (2012): a CNN would be **pre-trained on a large dataset like ImageNet**, then its weights would be **fine-tuned** on a smaller, domain-specific dataset (e.g. for object detection or medical images). This approach significantly improved performance over training a new model from random initialization. For instance, the 2014 R-CNN object detection model used an ImageNet-pretrained CNN (AlexNet) and fine-tuned it on detection data, demonstrating substantial gains in accuracy ([What is R-CNN?](https://blog.roboflow.com/what-is-r-cnn/#:~:text=Before%20warping%2C%20the%20region%20size,ImageNet%20for%20generic%20feature%20representation)). Fine-tuning allowed models to reuse low-level visual features learned from ImageNet and adapt the high-level layers to the new task, a paradigm that quickly became standard in vision.

In Natural Language Processing (NLP), transfer learning lagged behind vision until the late 2010s. Early NLP models mostly trained from scratch on each task, but the introduction of large **pre-trained language models** changed that. Approaches like ULMFiT (Howard and Ruder, 2018) showed that pre-training a language model on general text and fine-tuning it for a classification task could achieve state-of-the-art results with far less data. The watershed moment was **BERT** (Devlin et al., 2018), a Transformer model pre-trained on massive text via masked-language modeling. BERT’s release showed that a single pre-trained model could be **fine-tuned** for many NLP tasks (question answering, sentiment, NER, etc.) to achieve excellent performance ([Fine-tuning (deep learning) - Wikipedia](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)#:~:text=Models%20that%20are%20pre,6)). The fine-tuning of BERT was simple but powerful: Devlin et al. fine-tuned *all* model parameters on each downstream task, adding only a lightweight output layer, and obtained new state-of-the-art results on 11 different benchmarks ([BERT: Pre-training of Deep Bidirectional Transformers for Language ...](https://arxiv.org/abs/1810.04805#:~:text=,point)). This firmly established the transfer learning paradigm in NLP: a *pre-trained* model provides a general-purpose linguistic representation, and *fine-tuning* adapts it to specific tasks.

**Transfer learning and pre-trained models have had a profound impact.** Instead of training models from scratch (which requires huge data and compute), developers now leverage *foundation models* pre-trained on large corpora or datasets, and fine-tune them with relatively modest data and compute ([LLM Fine-tuning: Old school, new school, and everything in between - Fuzzy Labs](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between#:~:text=finetuning%20phase%20plays%20an%20important,for%20additional%20training)) ([LLM Fine-tuning: Old school, new school, and everything in between - Fuzzy Labs](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between#:~:text=Fine,LLM%20in%20the%20next%20section)). Fine-tuning not only yields higher accuracy (since the model starts from a strong baseline of knowledge) but also dramatically reduces the data requirement for the target task. This has made state-of-the-art results feasible even for teams with limited data or compute. Fine-tuning is typically a supervised learning process (using task-specific labeled data), but it can be combined with other techniques like **reinforcement learning with human feedback (RLHF)** for specialized adaptation (e.g. training ChatGPT by fine-tuning GPT-3 with RLHF) ([Fine-tuning (deep learning) - Wikipedia](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)#:~:text=Fine,9)). Overall, the evolution from training standalone models to **pre-train and fine-tune** has been a key driver of progress in modern AI.

## 2. Key Fine-Tuning Methodologies

Fine-tuning can be accomplished through different methodologies. The key approaches include **full model fine-tuning** (updating all weights), **partial fine-tuning** (updating some layers while freezing others), and a variety of **parameter-efficient tuning** techniques that introduce a small number of new parameters or modifications to avoid updating the entire model. Below we outline the major fine-tuning methods:

### Full Fine-Tuning (Entire Model)

**Full fine-tuning** means we take a pre-trained model and *train all of its parameters* on the new task data. Nothing is kept frozen – the model is optimized end-to-end for the downstream task. This was the approach used in the original BERT paper and remains a standard baseline. Full fine-tuning tends to yield the best task performance because it can *fully adjust the model’s knowledge* to the new data ([LLM Fine-tuning: Old school, new school, and everything in between - Fuzzy Labs](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between#:~:text=3,compared%20to%20the%20other%20two)). For example, fine-tuning all layers of a pre-trained CNN or Transformer typically outperforms leaving some layers frozen, especially when the downstream data size is large enough to avoid overfitting ([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models#:~:text=%2A%201%29%20Feature,test%20accuracy)).

However, full fine-tuning has drawbacks: (1) **Computational cost** – all weights (potentially billions of parameters) require gradient updates, so memory usage is high and training can be slow. With modern large models, updating every parameter is often infeasible on a single GPU. (2) **Storage and deployment** – a separately fine-tuned copy of the entire model must be saved for each task, which is inefficient when serving many tasks. (3) **Catastrophic forgetting** – without precautions, the model might overwrite or “forget” some of its pre-trained knowledge, especially if the new dataset is small; this can hurt generalization if not carefully regularized. Despite these issues, full fine-tuning is very effective when resources allow. It was the **go-to strategy in early deep learning transfer learning** and is still used when maximum performance is needed and model size is manageable.

A common variant is **partial fine-tuning**, where some layers (often the earlier layers) are frozen and only later layers are fine-tuned. This reduces computation and can avoid destabilizing low-level features. Freezing early layers (which capture generic features) while tuning higher layers (more task-specific features) is a heuristic that often works well ([Fine-tuning (deep learning) - Wikipedia](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)#:~:text=For%20some%20architectures%2C%20such%20as,4)). For instance, one might fix the first few convolutional layers of an ImageNet model and only fine-tune the top layers for a new image classification task. This *feature extractor* approach is fast and needs fewer parameters to update. Empirically, updating more layers tends to improve accuracy (as more capacity is adapted to the task), but with diminishing returns and higher cost ([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models#:~:text=%2A%201%29%20Feature,test%20accuracy)). The rule of thumb is to start with fewer trainable layers and progressively unfreeze more if the task performance plateaus.

### Adapter-Based Tuning (with Adapters and AdapterFusion)

Adapter-based tuning inserts small **adapter modules** into a pre-trained model’s layers, and only these new adapter weights are trained for the downstream task ([Fine-tuning (deep learning) - Wikipedia](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)#:~:text=which%20the%20parameters%20of%20a,3)). The original model’s weights remain *frozen*. This idea was popularized by Houlsby et al. (2019), who showed that by adding a few feed-forward layers (adapters) inside each Transformer block of BERT, one could achieve transfer learning performance almost as good as full fine-tuning while training only ~3.6% as many parameters per task ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=alternative%2C%20we%20propose%20transfer%20with,By%20contrast%2C%20fine)). In their design, each adapter is a tiny bottleneck MLP: it projects the layer’s hidden state down to a smaller dimension and then back up, and the output is added (residual) to the original hidden state. By training these adapter weights, the model can *learn task-specific transformations* without altering the original pre-trained weights ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=alternative%2C%20we%20propose%20transfer%20with,By%20contrast%2C%20fine)).

**Adapters** have several advantages: Each task’s adapters are lightweight (few million parameters), so multiple tasks can be served by one backbone model with different adapters. They also mitigate catastrophic forgetting, since the base model stays intact and task-specific knowledge is confined to the adapters. Houlsby et al. found that BERT with adapters attained within 0.4% of full fine-tuning performance on the GLUE benchmark while being far more parameter-efficient ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=and%20new%20tasks%20can%20be,of%20the%20parameters%20per%20task)). Subsequent works (e.g. Pfeiffer et al. 2020) introduced variations like **Pfeiffer adapters** (slightly different placement or architecture) and an ecosystem (AdapterHub) for sharing pre-trained adapters.

An extension of this idea is **AdapterFusion** (Pfeiffer et al., 2021), which addresses learning from multiple tasks. In AdapterFusion, one first trains separate adapters for each task (or each data domain), and then learns a *fusion layer* that composes the knowledge from a set of these adapters without destroying them ([[2005.00247] AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247#:~:text=%3E%20Abstract%3ASequential%20fine,classifier%20can%20effectively%20exploit%20the)) ([[2005.00247] AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247#:~:text=empirically%20evaluate%20AdapterFusion%20on%2016,code%20and%20adapters%20are%20available)). The fusion module learns how to weight or combine the outputs of multiple task adapters. This allows **non-destructive task composition** – for example, combining adapters from Tasks A, B, C to handle a new Task D that has related aspects of each. Pfeiffer et al. showed that AdapterFusion can outperform multi-task fine-tuning, effectively leveraging knowledge from diverse tasks while keeping the original adapters fixed ([[2005.00247] AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247#:~:text=%3E%20Abstract%3ASequential%20fine,classifier%20can%20effectively%20exploit%20the)) ([[2005.00247] AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247#:~:text=empirically%20evaluate%20AdapterFusion%20on%2016,code%20and%20adapters%20are%20available)). This is useful in practical settings where you might incrementally add new tasks and want to reuse existing adapters rather than fine-tune the whole model again (which could forget previously learned tasks).

In practice, adapter-based tuning requires modifying the model architecture to include adapter modules. Libraries like **Adapter-Transformers** (AdapterHub) integrate this with Hugging Face models, providing functions to add adapters to a model and train them ([Quick Start — AdapterHub documentation](https://docs.adapterhub.ml/quickstart.html#:~:text=,examples%20to%20showcase%20these%20methods)). Adapters can be inserted after the self-attention and/or feed-forward sub-layers of each Transformer block. The size of the adapter (bottleneck dimension) is a key hyperparameter controlling the trade-off between capacity and efficiency. Overall, adapter-based fine-tuning achieves strong performance with high parameter efficiency, making it attractive for scenarios like multi-task learning or deploying many task-specific models without duplicating the entire weight set for each.

### Low-Rank Adaptation (LoRA) and QLoRA

**Low-Rank Adaptation (LoRA)** is a fine-tuning method that injects trainable low-rank updates into the model’s weight matrices, instead of adding full layers. Introduced by Hu et al. (2021), LoRA targets the large weight matrices (e.g. the query/key/value or feed-forward projection matrices in Transformers) and represents the change to these weights as a low-rank decomposition ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=LoRa%20Hu%20et%C2%A0al,rank%20matricies)) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=def%20lora_linear)). For a given weight matrix $W$ of size $m\times n$, LoRA learns two much smaller matrices $W_A$ (size $m\times r$) and $W_B$ ($r\times n$) such that the **update** to the original weight can be written as $ΔW = W_A W_B$ (with $r \ll m,n$). During fine-tuning, $W_A$ and $W_B$ are the only trainable parameters (along with a possible scaling factor) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=LoRa%20Hu%20et%C2%A0al,rank%20matricies)). The original $W$ stays frozen. At inference time, the low-rank product $W_AW_B$ can be added to $W$, or merged for efficiency.

LoRA is appealing because a low-rank approximation can capture task-specific changes with far fewer parameters than the full matrix. For example, if $r=8$ in a 2048-dimensional matrix, the LoRA parameters are a tiny fraction (~0.4%) of the original. Despite this, LoRA often matches or even outperforms other adapter methods. Hu et al. demonstrated LoRA on very large models (up to 175B GPT-3) and found it to be effective and stable ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=In%20Transformers%2C%20LoRa%20is%20typically,models%20up%20to%20175B%20parameters)). In fact, LoRA fine-tuning of GPT-3-style models achieved results on par with full fine-tuning while training only ~0.1%-0.3% of the parameters. One reason for its success is that large language models may have a low intrinsic dimension for many tasks – a low-rank update is sufficient to nudge the model’s behavior. Another advantage is that LoRA updates can be merged into the base model weights after training, incurring *zero* runtime overhead (or one can keep them separate to easily enable/disable the fine-tuning). Empirically, LoRA tends to outperform classic adapters and simple bias-tuning on many benchmarks ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=In%20Transformers%2C%20LoRa%20is%20typically,models%20up%20to%20175B%20parameters)), making it one of the **most popular PEFT methods** for large LMs.

A recent development is **QLoRA (Quantized LoRA)** by Dettmers et al. (2023). QLoRA tackles the *memory bottleneck* of fine-tuning huge models by first **quantizing the pre-trained model to 4-bit** precision, and then applying LoRA on top of that compressed model ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)). In QLoRA, the forward and backward passes are done with a 4-bit weight representation (using a special quantization format called NF4 with double quantization of outlier values), drastically reducing memory usage, while LoRA handles the weight updates in higher precision ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=preserving%20full%2016,memory%20footprint%20by%20quantizing%20the)). Using QLoRA, Dettmers et al. were able to fine-tune a 65-billion-parameter LLaMA model *on a single 48GB GPU*, something infeasible with normal 16-bit fine-tuning ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)). Impressively, QLoRA preserved full 16-bit fine-tuning performance – their fine-tuned 65B model (named Guanaco) reached ~99% of ChatGPT’s quality on a benchmark, with just 24 hours of training ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=preserving%20full%2016,memory%20footprint%20by%20quantizing%20the)) ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=Rank%20Adapters,providing%20a%20detailed%20analysis%20of)). QLoRA thus combines quantization and LoRA to push the limits of efficiency: it requires orders-of-magnitude less memory while maintaining accuracy. It has enabled many researchers to fine-tune large models on affordable hardware. The success of QLoRA suggests that aggressive compression of the base model (for example 4-bit weights) is compatible with effective low-rank adaptation, as long as the quantization errors are managed properly (via techniques like NF4 and double quantization in this case).

In summary, LoRA and QLoRA represent a powerful class of fine-tuning methods where we **train a small low-rank “delta”** for each large weight matrix. They are highly efficient in terms of trainable parameters and memory, and have quickly become go-to methods for fine-tuning large language models (LLMs) like GPT-family, LLaMA, OPT, etc. The approach is supported in frameworks such as Hugging Face’s PEFT library for easy integration into model training.

### Prefix Tuning and Prompt Tuning

**Prompt tuning** and **prefix tuning** are techniques that fine-tune *input prompts* or *prefix activations* rather than the model’s main weights. These methods stem from the observation that large pre-trained models can perform tasks if given appropriate prompts; thus, instead of changing the model, one can change the prompt that is fed to it.

**Prompt tuning** (sometimes called “soft prompt tuning”) involves learning a set of *virtual tokens* or embeddings that, when prepended to the input, coax the model into solving the desired task. Lester et al. (2021) introduced prompt tuning for GPT-2 and T5 models, showing that one can keep the model frozen and just optimize a short learned prompt vector for each task ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=any%20number%20of%20labeled%20examples,2021)). In practice, this means we create a sequence of $k$ learnable embedding vectors (not corresponding to real words) and prepend them to every input. The model then processes input as if it began with those $k$ tokens, and the prompt embeddings are trained via backpropagation on the task loss. Remarkably, for very large models (with billions of parameters), prompt tuning can achieve performance close to fine-tuning the entire model ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=any%20number%20of%20labeled%20examples,2021)). Lester et al. found a clear trend: *prompt tuning becomes more competitive as model size grows*, matching full-model tuning on T5-XXL (11B) but lagging on smaller models ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=any%20number%20of%20labeled%20examples,2021)). This suggests that large models have enough capacity and general knowledge that steering them with a learned prompt is sufficient for many tasks, whereas smaller models need internal parameter updates to adapt effectively.

**Prefix tuning** (Li and Liang, 2021) is a related idea but operates at the **hidden-state level** of each Transformer layer ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=Li%20and%20Liang%20,prompt%20to%20the%20model%20input)). Instead of optimizing input embeddings, prefix tuning injects learnable vectors at each layer’s input to the self-attention (effectively prepending a series of $k$ key-value pairs to the attention computation). The model is extended with a set of free parameters (“prefixes”) that serve as an additional context for every layer, and only these prefix tensors are trained ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=7.2%20Prefix)). Conceptually, this means we provide the model with a trainable “context latent” at every layer, guiding its computation for the task. Li and Liang found this approach more stable than directly tuning input embeddings for generation tasks, and they used a small feed-forward network to generate the prefixes, which improved training stability ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=Li%20and%20Liang%20,prompt%20to%20the%20model%20input)). Prefix tuning achieved strong results on GPT-2 for tasks like table-to-text generation with only 0.1%-3% of parameters tuned ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=Prefix,5%20yes%20no%20no)).

The strengths of prompt/prefix tuning are extreme parameter efficiency (only the prompt vectors are learned, which can be as low as a few thousand parameters) and the ability to reuse one frozen model for many tasks by storing different learned prompts ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=on%20model%20size%20using%20T5%2C,soft%20prompts%20confers%20benefits%20in)). Also, by not modifying the model weights, we avoid catastrophic forgetting of the base capabilities. Another benefit is modularity: one can “prompt merge” or ensemble by concatenating prompts for multitask use, in some research works. Prompt-based tuning is also appealing for scenarios where one has API access to a model (as the prompt can be adjusted without retraining the model itself).

On the downside, prompt tuning sometimes requires careful initialization or sufficient prompt length to be effective, and may underperform full fine-tuning on smaller models or tasks requiring more significant model changes. It can be *less robust in low-data regimes* because the prompt has limited expressive power. In such cases, hybrid approaches or a few additional parameters (like tuning some layer norms or biases alongside the prompt) can help. Nonetheless, prompt and prefix tuning are important tools, especially with the trend of deployment where models might be hosted behind APIs or one might not want to copy the model for each task. They demonstrate an extreme end of *“fine-tuning” without modifying model weights*, instead learning task-specific *inputs* to elicit the right behavior.

### Parameter-Efficient Fine-Tuning (PEFT) in General

Collectively, methods like adapters, LoRA, prefix tuning, and prompt tuning are often referred to as **Parameter-Efficient Fine-Tuning (PEFT)** techniques. The common goal of PEFT is to **adapt large pre-trained models using only a small number of trainable parameters**, addressing the infeasibility of full fine-tuning for very big models ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=This%20paper%20presents%20a%20systematic,scale%20language%20models)) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=In%20October%202018%2C%20BERT%20Large,%28%206%29%3B%20Shoeybi)). PEFT methods allow scaling to billions of parameters by avoiding the need to compute gradients for all weights. A recent survey identified *over 30 PEFT methods* proposed between 2019 and 2023 ([LLM Fine-tuning: Old school, new school, and everything in between - Fuzzy Labs](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between#:~:text=training%20only%20a%20subset%20of,On%20a)) – indicating how rapidly this research area has expanded in the era of large language models.

PEFT approaches can be categorized by *where* and *how* they introduce trainable parameters: some (like adapters and prefix tuning) add new parameters (layers or vectors) into the model’s architecture; others (like BitFit) choose a subset of existing parameters to tune (e.g. only biases); others (like LoRA) reparameterize existing weights in a factored form ([Adapters](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#:~:text=Adapter,larger%20models%20with%20fewer%20resources)). The unifying theme is that only a *small fraction* of the model’s parameters are updated, often on the order of <1% of the total. Despite this drastic reduction, many PEFT methods reach performance comparable to full fine-tuning on various tasks ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=scale.%20Parameter,tuned%20models)) ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=State)). This is possible because large pre-trained models are highly overparameterized for most specific tasks, so a few well-chosen parameter adjustments can effectively unlock the necessary knowledge.

Benefits of PEFT include: lower memory usage, faster training, and the ability to maintain a single frozen model serving multiple tasks (just swap in different efficient tuners) ([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models#:~:text=Parameter,for%20at%20least%205%20reasons)) ([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models#:~:text=4,overfitting)). It also often reduces the risk of overfitting on small datasets, since fewer parameters means a form of regularization ([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models#:~:text=efficient%20finetuning%20is%20useful%20for,at%20least%205%20reasons)). Many PEFT methods also retain the nice property that the original model’s weights are unchanged, so the model’s general abilities remain intact and can be reverted to or combined as needed.

Given the strong interest, tools like Hugging Face’s `peft` library have emerged to unify PEFT techniques under a common API. This library implements popular methods (LoRA, prefix tuning, prompt tuning, IA3, etc.) and makes it easy to wrap a pre-trained model for efficient fine-tuning ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=State)) ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=PEFT%20is%20integrated%20with%20Transformers,inference%20for%20really%20big%20models)). In practice, choosing a PEFT method involves considering the task type, model, and resource constraints. For example, LoRA is widely used for generative LMs, prefix tuning for text generation tasks, and adapters for cases requiring modular training across tasks.

### BitFit (Bias-Only Fine-Tuning)

**BitFit** (Ben-Zaken et al., 2021) is a particularly simple PEFT approach: it fine-tunes **only the bias terms** of the model’s layers, and leaves all weight matrices frozen ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic)). Neural network layers (dense, convolution, etc.) often have far fewer bias parameters than weights, so this is an extreme reduction – e.g. in a Transformer with tens of millions of weights per layer, only a few thousand biases (one per hidden unit) are trainable. The intuition behind BitFit is that adjusting biases shifts the neuron activation thresholds and can coarsely adapt how information flows, possibly enough to learn a new task in many cases.

Surprisingly, the BitFit paper found that for *small-to-medium datasets*, tuning just the biases of a pre-trained BERT was **competitive with full fine-tuning** on GLUE tasks ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic)). In some instances it even slightly outperformed full fine-tuning, likely because the massive reduction in trainable parameters acted as regularization and prevented overfitting ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic)). On larger data, BitFit did somewhat worse than full fine-tuning, but remained on par with other lightweight methods. These results support a hypothesis that much of what fine-tuning does is re-weight existing knowledge rather than create new representations ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=competitive%20with%20,specific%20linguistic%20knowledge)) – in other words, by just nudging biases, the model can “expose” the features it already learned during pre-training to solve the task ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=competitive%20with%20,specific%20linguistic%20knowledge)).

The main advantage of BitFit is its **simplicity and efficiency**: one can implement it by simply zeroing out all gradient requires for weights except bias tensors. It requires the least amount of new parameters or architectural changes (none at all beyond allowing biases to train). For deployment, a BitFit “delta” is just the set of bias vectors, which is extremely small to store and apply. This also means one can maintain a single base model and have per-task bias sets to swap in.

The obvious disadvantage is that bias-only tuning is less expressive than other methods; some tasks might require more significant transformations than a bias shift. If the task is very different from what the model was pre-trained on, BitFit might not suffice. In practice, BitFit often yields a modest drop in performance compared to full fine-tuning (especially on complex tasks or with abundant data), but it’s a useful baseline and can be combined with other lightweight tweaks (for instance, some have tried BitFit+LayerNorm tuning, etc., which still keeps parameter count low).

### Other Emerging Fine-Tuning Techniques

Beyond the mainstream methods above, there are numerous emerging fine-tuning techniques. A few notable ones include:

- **Hybrid or Composite Methods:** Researchers have experimented with combining multiple PEFT approaches. For example, **UniPELT** (Mao et al., 2022) is a unified framework that applies a mixture of LoRA, prefix tuning, and adapters simultaneously, with gating to decide which method to use at each layer ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=UniPELT%20Mao%20et%C2%A0al,layer%20of%20the%20transformer%20block)) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=and%20Prefix%20Tuning%20approaches%20in,1B%20parameters%29%20models)). Similarly, techniques like **Stacked Adjacent Fine-tuning** (S4) use automated searches to find optimal combinations of adapters, prompts, etc. ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=both%20Adapters%20and%20Prompt%20tuning,search%20that%20combines%20all%20PEFT)). These aim to get the best of all worlds, albeit with added complexity.

- **Compactor and Kronecker Adaptation:** *Compacter* (Mahabadi et al., 2021) introduced the idea of using low-rank *and* weight-sharing in adapters, employing hypercomplex multiplication (PHM) to further reduce parameters. A related approach is **KronA** (Edalati et al., 2022), which replaces LoRA’s simple low-rank product with a structured Kronecker product update for each weight matrix ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=10)) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=x%20%3D%20x%20%40%20W,regular%20linear)). KronA achieves a better parameter-efficiency trade-off and faster inference (since the Kronecker structure can be exploited) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=KronA%20is%20another%20method%20presented,weights%20and%20a%20residual%20connection)), though it’s been tested mostly on smaller models so far.

- **Sparse Fine-Tuning and Pruning:** Instead of selecting a subset of parameter *types* to train (like biases), some methods learn a subset of parameter *values* to train. For instance, **Diff Pruning** (Guo et al., 2021) learns a sparse mask on the weight differences – essentially learning which individual weights to change for the task. Others have looked at lottery-ticket style pruning during fine-tuning to activate a small subnetwork. These approaches can achieve good efficiency, but sparse operations are not always hardware-friendly, making them less popular in practice at the moment.

- **Hypernetwork and Meta-tuning:** Another direction is using a small **hypernetwork** that generates task-specific weights for the main model. For example, one could train a hypernetwork that takes a task identifier or some task description and outputs a LoRA update or adapter weights for the big model. This can allow multi-task adaptation without storing separate adapters for each task, and even generalize to new tasks to some extent. However, training such hypernetworks can be complex and is an ongoing research area.

- **In-context Fine-tuning / Meta-learning:** Moving beyond gradient-based fine-tuning, techniques like **prompt engineering** or **in-context learning** (not actually training, but worth noting) allow a form of “on the fly” adaptation by supplying demonstrations in the prompt. Also, meta-learning algorithms (e.g. MAML) attempt to initialize models such that they can be fine-tuned with very few examples. These are tangential to classic fine-tuning but contribute to the toolkit of adapting models to new tasks.

Overall, the landscape of fine-tuning techniques is rapidly evolving, especially with the focus on making **LLM adaptation efficient**. The methods listed above (full vs. adapters vs. LoRA vs. prompts vs. biases, etc.) each represent a different balance between *effectiveness*, *efficiency*, and *implementation complexity*. In the next section, we will compare these methods along key dimensions.

## 3. In-Depth Explanation of Representative Methods

In this section, we delve a bit deeper into the theoretical foundation and practical functioning of some of the most relevant fine-tuning methods, highlighting their strengths and weaknesses:

- **Full Fine-Tuning (and Partial Fine-Tuning):** *Theory:* This is essentially standard supervised learning applied to a pre-trained model. The pre-trained weights serve as initialization, and gradient descent updates all (or most) of them to minimize the task loss. Theoretically, if the pre-trained model has learned a good representation, fine-tuning can be viewed as finding a nearby minima in weight space that is suitable for the new task. Fine-tuning can also be seen through the lens of **catastrophic forgetting** and **plasticity-stability balance** – updating too much can overwrite useful features, so sometimes practitioners use smaller learning rates or freeze certain layers to maintain stability ([Fine-tuning (deep learning) - Wikipedia](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)#:~:text=For%20some%20architectures%2C%20such%20as,4)). *Practice:* Full fine-tuning is straightforward to implement and often yields top performance, especially when the downstream dataset is large. It has been successfully used in countless settings (CV, NLP, speech) – whenever someone says “we fine-tuned the model on our data,” they usually mean this. *Strengths:* Highest task performance in many cases; no changes to model architecture; can correct any deficiency of the pre-trained model since all parameters can be adjusted. *Weaknesses:* Not feasible for very large models without enormous compute/memory; requires storing a full copy of weights per task; risk of overfitting or forgetting if not carefully done. In comparative studies, full fine-tuning is the yardstick that other methods try to match with fewer trainable parameters ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=and%20new%20tasks%20can%20be,of%20the%20parameters%20per%20task)). It often wins when one has abundant data and compute, but loses on efficiency and multi-task adaptability.

- **Adapter-Based Fine-Tuning:** *Theory:* Adapters work on the principle of **keeping the original model as a fixed feature extractor** and learning task-specific “side” mappings. By introducing a bottleneck, they force the task adaptation to occur in a limited-dimensional subspace, which tends to be enough if the pre-trained model’s features are broad and expressive. From a theoretical perspective, adapters constrain the optimization problem (less parameters to solve for) which can be seen as adding a regularization that the solution must lie in the space spanned by adapter transformations. *Practice:* In implementation, adapters are typically two linear layers with a non-linearity in between (like a mini feed-forward network) inserted at certain points (e.g., after the self-attention output and after the feed-forward output in a Transformer block, per Houlsby et al.). During training, only these mini-layers’ weights get gradients. At inference, the adapter adds a small computational overhead per token (e.g., a couple of additional matrix multiplies). The overhead is modest, but if a model has many adapters for different tasks, one typically “activates” only the adapter for the current task. Modern libraries allow one to *stack multiple adapters* in a model and select which to use, or even use multiple at once via fusion. *Strengths:* Very parameter-efficient (small per-task memory). Avoids interference with base model weights, so one model can serve many tasks by loading different adapters. Also, adapters can often be trained faster than full fine-tuning since there are fewer parameters (and sometimes one can get away with a larger learning rate on them due to their limited scale). They have been shown to work across many modalities (there are adapters for vision models, multilingual models, etc.). *Weaknesses:* There is some *drop in performance* relative to full fine-tuning in certain cases (Houlsby et al. reported a tiny drop of 0.4% on GLUE ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=and%20new%20tasks%20can%20be,of%20the%20parameters%20per%20task)); other tasks might see a bit more). The model’s capacity for task-specific adaptation is limited by the adapter’s size – if an adapter is too small (too low bottleneck rank), it might underfit the task. Also, implementing adapters means modifying the architecture or using a specialized library, which adds complexity (though it’s well-supported now). Inference speed is slightly reduced because of the extra computations, but this is usually negligible compared to the rest of the model. In summary, adapters are a **workhorse method** that achieve near state-of-the-art performance with a tiny fraction of trainable parameters, at the cost of a minor architecture augmentation.

- **LoRA (Low-Rank Adaptation):** *Theory:* LoRA is grounded in the idea that the change needed to adapt a model to a new task might lie in a low-dimensional subspace of the full weight space. By constraining $ΔW = W_A W_B$ with $W_A \in \mathbb{R}^{m \times r}, W_B \in \mathbb{R}^{r \times n}$, we are essentially saying: “rather than learning $m \times n$ parameters for the weight update, we believe the update has rank at most $r$.” This greatly reduces parameters and also imposes a *structure* on the update which can act as regularization. In linear algebra terms, we’re projecting the gradient updates onto a low-rank manifold. *Practice:* Implementing LoRA typically involves identifying which weight matrices to target (e.g., the query and value projection matrices in attention are common targets) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=In%20Transformers%2C%20LoRa%20is%20typically,models%20up%20to%20175B%20parameters)). For each such weight $W$, we create two new parameter matrices $W_A, W_B$ of chosen rank $r$. During forward pass, we compute the normal $W x$ and also $W_B(W_A x)$ which is the low-rank adjustment, and add them together (sometimes scaled by a factor) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=%E2%AC%87)). Only $W_A$ and $W_B$ receive gradients. The original $W$ is unchanged (frozen). After training, one can either keep performing the forward in this way or **merge** the weights: since $W + ΔW$ can be computed and stored, one could integrate the LoRA update into $W$ permanently for deployment. *Strengths:* Extremely parameter-efficient; even more so than adapters in many cases because a rank-$r$ update may have fewer parameters than an adapter with an internal dimension of similar size. No significant inference slow-down if merged (and if not merged, the extra matrix multiply is not too bad either). LoRA has been shown to outperform some other PEFT methods – for example, in the "Scaling Down to Scale Up" study, LoRA generally performed as well or better than adapters and bias-tuning on language tasks ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=In%20Transformers%2C%20LoRa%20is%20typically,models%20up%20to%20175B%20parameters)). It’s also very **easy to implement** relative to full adapters, since it’s essentially a modification inside a linear layer. Another plus: because LoRA doesn’t add new non-linearities (it’s just a linear combination), it can be easier to merge and analyze. *Weaknesses:* One limitation is that LoRA assumes a fixed low rank; if the true complexity of the task update is higher, a too-small $r$ will hurt performance. (However, one can choose $r$ fairly flexibly; e.g. r=4,8,16 are common and usually sufficient for many tasks.) LoRA also needs careful tuning of which layers to apply it to – applying LoRA to every matrix in a model is possible but might be overkill, whereas applying to too few might limit performance. In practice, LoRA is often applied primarily to attention layers. Additionally, while LoRA itself is very efficient in terms of *number* of parameters, those parameters are unstructured dense matrices $W_A$ and $W_B$. If one is extremely memory-constrained, even storing those for a 175B model for many tasks could add up (though still far, far less than storing full models). Overall, LoRA’s theoretical simplicity and strong empirical performance on large LMs have made it one of the most **widely adopted fine-tuning methods**, especially for the current generation of large language models where full fine-tuning is impractical.

- **Prompt/Prefix Tuning:** *Theory:* These methods leverage the model’s existing capacity by providing learned input signals. From a theoretical standpoint, prompt tuning can be viewed as finding an optimal *embedding vector* in the model’s input space that triggers the right neurons for the task at hand. It is related to the concept of **conditioning** – we are not changing the model, but conditioning it on a task description represented by the learned prompt. In a sense, the model “reads the prompt” and internally configures itself in a way that solves the task. Prefix tuning extends this idea deeper into the model, which can be seen as providing a form of learned **context** at every layer, akin to learned hidden activations that remain fixed for the task. Li and Liang (2021) theorized that optimizing a prefix directly can be unstable, so they use a small neural network to generate the prefix (this is like a mini model that produces the prompt vectors, adding a bit more capacity and smoothness to the optimization) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=Li%20and%20Liang%20,prompt%20to%20the%20model%20input)). *Practice:* Prompt tuning is very simple to implement if your framework allows gradient updates to the token embeddings. One simply creates $k$ new tokens (often initialized to random or copied from some average embedding) and concatenates them to each input. Only those $k$ token embeddings are set to `requires_grad=True`. Everything else, including the original vocabulary embeddings for real tokens, stay fixed. Training proceeds as usual; the gradients flowing into the prompt embeddings update them. For prefix tuning, implementation is a bit more involved: one needs to create learnable key/value tensors for each Transformer layer’s attention mechanism. Some frameworks (like 🤗 PEFT) provide this, where you specify a `PrefixTuningConfig` with the prefix length, and it will augment the model accordingly. These learnable prefixes are typically initialized small (maybe even zero or random) and trained. In both cases, after training, you keep the prompt vectors around and prepend them during inference (for tasks like classification, generation, etc., by attaching them to input or as a fixed prefix at each layer). *Strengths:* The most obvious strength is **minimal intrusion** – you don’t have to alter the model weights at all. This is great for using hosted API models or scenarios where you cannot retrain the model fully. Also, the *footprint* of a prompt is tiny (just a few KB potentially), so it’s trivial to store or share. You can maintain one frozen model and many prompts for different tasks in memory without issue. Another interesting strength is that prompt tuning sometimes improves robustness and domain transfer. Lester et al. noted that conditioning a frozen model with prompts can confer benefits in domain generalizatio ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=matches%20the%20strong%20performance%20of,checkpoints%20to%20reproduce%20our%20experiments))】 (perhaps because the base model’s general knowledge isn’t distorted). Prompt ensembling is also an option: you can train multiple prompts and combine their outputs or even their embeddings for possibly better performance, which is a unique lever that weight-tuning methods don’t directly have. *Weaknesses:* The main weakness is that prompt tuning usually requires *very large models* to work really wel ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=any%20number%20of%20labeled%20examples,2021))】. With smaller models, prompt tuning often underperforms, sometimes substantially. It relies on the pre-trained model being smart enough that a fixed prompt can elicit the needed behavior. If the model doesn’t already kind of “know” how to do the task, a prompt won’t magically make it learn new facts – it can only steer existing capabilities. Thus, prompt tuning is not well-suited when significant novel learning is required. Also, prompt tuning can be *task-sensitive*: some tasks (especially generative ones or tasks where the output is a textual response) are more amenable than others. For classification tasks, prompt tuning might need careful design (some people use a template like “ Text: [input]. Answer: [label]” with a soft prompt for ). If done naively, it could be less straightforward than fine-tuning a classifier head. Additionally, optimization may require more steps or careful LR scheduling, since only a small set of parameters must compensate for all needed changes. In summary, prompt and prefix tuning are **powerful for efficiency and model sharing**, but one has to accept a potential hit in accuracy for smaller models and possibly more hyperparameter tuning to get it right. They shine in scenarios where one cannot or will not touch model weights, and in the limit of very large models, they achieve results surprisingly close to full fine-tuning.

- **BitFit (Bias Tuning):** *Theory:* BitFit can be thought of as a highly constrained optimization: we only allow bias terms $b$ in layers like $y = Wx + b$ to be updated. A bias term adds a constant (for each neuron) to the output, so effectively BitFit is tuning how easily each neuron/unit fires or how it thresholds inputs, without changing the core weight that connects inputs to that neuron. The hypothesis behind BitFit is that pre-trained weights $W$ encode a lot of features, and maybe all we need for a new task is to *linearly shift* the outputs of some neurons (via bias) to activate different feature combinations. This aligns with the view that fine-tuning largely *recombines existing features* (which bias shifts can facilitate) rather than creating new one ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=competitive%20with%20,specific%20linguistic%20knowledge))】. *Practice:* BitFit is extremely easy: set `requires_grad=False` on all parameters except biases. In many deep learning frameworks, biases are identifiable by name (e.g., “bias” in PyTorch layer names). Then train normally. Because so few parameters are being updated, one might increase the number of training epochs or use a relatively higher learning rate for those biases (they won’t destabilize as much as if you were moving all weights). Monitoring is also simpler: you could watch how bias values move from their pre-trained values to see which ones change a lot (indicating which neurons are being repurposed most for the task). *Strengths:* As discussed, it’s extremely efficient in memory and compute. It also tends to be more *robust to overfitting* in low-data cases than full fine-tuning – in some experiments, BitFit outperformed full fine-tuning when training data was limite ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))】, presumably because full fine-tuning over-parameterized the problem and overfit, whereas bias-only had an implicit regularization. BitFit is also additive: you can imagine training bias offsets for many tasks and applying them on top of the same base model without conflict (as long as you swap the biases accordingly). Swapping out biases is as trivial as swapping an adapter. *Weaknesses:* The capacity of BitFit is obviously limited. If a task requires, say, some feature that wasn’t present in the pre-trained model at all, bias tuning can’t manifest that out of thin air. In practice, with large Transformers, most tasks still see a drop in accuracy with BitFit compared to full fine-tuning, especially for complex outputs. For instance, on GLUE benchmarks with ample data, BitFit was generally a few points behind full fine-tuning (though it narrowed the gap for smaller data ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))】. Another weakness is that biases exist in certain places only – e.g., in linear and convolution layers, yes, but not all operations have a bias (e.g., layer normalization has gain and bias, which one could include, and attention mechanisms often have some biases). BitFit authors typically include all bias terms model-wide. It’s possible that some networks could even be set up without biases (though most have them), in which case BitFit would have nothing to tune! Lastly, one might combine BitFit with training a small output head (say a classification vector) because if you strictly only tune biases inside the model, you still need to produce task-specific output somehow. Usually, the classification head’s weights are small and can be included or one can cast that as biases too if structured right. In conclusion, BitFit is a **minimalist fine-tuning** approach – very useful when you need ultra-light adaptation or as a baseline to test how much improvement full fine-tuning is giving beyond simply shifting neuron biases. It often sets a surprisingly strong baseline on NLP task ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))】, confirming that small tweaks to a big pre-trained model can go a long way.

Each of these methods has a solid theoretical motivation and practical evidence backing it. Researchers continue to refine these ideas – for example, there are investigations into *which layers* are best to apply LoRA or adapters (sometimes the top and bottom layers might not need them), or whether we can learn the optimal subnetwork to fine-tune (lottery tickets for fine-tuning). The choice of method can depend on requirements: if you need absolute best accuracy and have resources, full fine-tune might be best; if you need to deploy many models or handle a 175B parameter model, LoRA or adapters or prompt tuning becomes essential.

## 4. Comparative Analysis of Fine-Tuning Methods

Fine-tuning methods can be compared along several axes: **computational cost**, **memory usage**, **scalability to large models**, **task performance**, and **practical considerations** like ease of implementation and versatility. Below, we analyze differences among the methods discussed:
([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models))】 *Figure: Schematic comparison of fine-tuning strategies in terms of modeling performance vs. training efficiency. “Feature-based” (only new classifier, base model frozen) is fastest but yields lower performance, “Finetuning I” (fine-tune last few layers) is a middle ground, and “Finetuning II” (full model fine-tuning) is slowest but typically achieves the best performanc ([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models#:~:text=%2A%201%29%20Feature,test%20accuracy))】. This illustrates the classic trade-off: more layers tuned = better accuracy but higher computational cos ([Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models#:~:text=These%20results%20are%20consistent%20with,it%20comes%20with%20increased%20cost))】.*

- **Memory Usage (Training)**: Full fine-tuning is the most memory-intensive because one must store gradients and optimizer states for all parameters. For a model with *N* parameters, an optimizer like Adam might use ~8N extra bytes (for momentum and variance), plus N for gradients, etc. This becomes prohibitive as N grows into billion ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=In%20October%202018%2C%20BERT%20Large,%28%206%29%3B%20Shoeybi)) ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=models%20grew%20up%20to%20176,most%20and%20impractical%20for%20everyone))】. In contrast, PEFT methods drastically cut the number of trainable parameters *p*, so memory for gradients/optimizer is proportional to *p* rather than *N*. For example, if LoRA tunes 0.1% of parameters, you cut memory overhead by ~1000×. Adapters usually amount to a few percent of *N* (e.g. 3.6% in Houlsby’s setu ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=and%20new%20tasks%20can%20be,of%20the%20parameters%20per%20task))】), still a huge reduction. Prompt tuning might be on the order of just 0.01% of *N* or less (for a prompt of a few hundred tokens). BitFit trains maybe ~0.1–0.5% of *N* (since biases are much fewer than weights). Thus, methods rank roughly: *Full FT >> partial FT > adapters ≈ LoRA > BitFit > prompt tuning* in terms of training memory needed. QLoRA provides an extra boost by reducing memory for the *stored model* via 4-bit quantization, making even *N* itself smaller at training tim ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=preserving%20full%2016,memory%20footprint%20by%20quantizing%20the))】. In effect, QLoRA has the lowest memory footprint among approaches for giant models: it trains ~0.1% of parameters and keeps the rest in 4-bit, enabling fine-tuning of a 65B model on a 48GB GP ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is))】.

- **Computation and Speed:** There are two aspects: forward pass cost and backward pass cost. In forward passes, full fine-tuning and BitFit are identical to the base model (no structure change), so they have the same FLOPs per token. Adapters add a small number of additional operations (a couple dense layers per transformer block) – typically negligible relative to the whole model, maybe a 5-10% overhead at most. LoRA adds an extra matmul add in forward, again quite negligible (and if merged after training, then zero overhead). Prefix tuning adds some extra operations for the prefix (computing attention with additional key/value vectors), which can be a small overhead especially if the prefix length is not too large (e.g. 10–20). Prompt tuning adds actual tokens to the sequence, so if your prompt is, say, 20 tokens, every input is 20 tokens longer – this can be a slight slow-down in proportion to prompt length vs. original sequence length. So at inference, all these methods are roughly similar, often within 0–10% of the base model runtime (with the exception of extremely long prompts or extreme adapter architectures). During backward pass (training), the cost is proportional to how many parameters get gradients. Surprisingly, even if you only train, say, 1% of the parameters, you might not get a full 100× speedup, because backprop still has to flow through the rest of the model (computing gradients for those weights even if you don’t store them). However, many frameworks can skip gradient calculation for frozen weights, which saves some time. Typically, PEFT methods do yield some training speedups due to less gradient computation and simpler optimization. For example, updating only biases or a small adapter can reduce the time per step (and allows larger batch sizes for the same memory). The *wall-clock speed* also depends on implementation details – e.g., highly optimized kernels exist for dense matmuls (full fine-tuning), whereas a custom adapter might not be as well-optimized and could incur overhead from Python layers. In large-scale setups, though, the memory constraint is usually the bottleneck; by alleviating it, PEFT can actually allow using bigger batch sizes or sequence lengths, indirectly improving throughput.

- **Effectiveness vs. Model Size:** As noted, prompt tuning’s effectiveness increases with model siz ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=any%20number%20of%20labeled%20examples,2021))】 – with very large models (billion+ parameters), prompt or prefix tuning can match full fine-tuning on many tasks, but on smaller models (hundreds of millions or less) they often underperform. LoRA and adapters tend to be effective across a range of model sizes, but still, if a model is very small (say a 10M parameter model), tuning only a tiny fraction might not give enough capacity to learn the task. In such cases, full fine-tuning or at least tuning a larger portion of weights might be needed. Conversely, for **extremely large models** (think 100B+), full fine-tuning is typically impossible for most people, and methods like LoRA, QLoRA, and adapters become *enabling technologies* – they make it feasible to adapt those models at al ([LLM Fine-tuning: Old school, new school, and everything in between - Fuzzy Labs](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between#:~:text=One%20major%20drawback%20with%20using,to%20train%20all%20the%20layers)) ([LLM Fine-tuning: Old school, new school, and everything in between - Fuzzy Labs](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between#:~:text=Consider%2C%20for%20example%2C%20the%20task,to%20train%20all%20the%20layers))】. Another aspect is **architecture**: certain architectures may benefit more from one method or another. For instance, GPT-style causal LMs have been a sweet spot for LoRA and prefix tuning (common in LLM fine-tuning), whereas BERT-style encoders for classification have seen a lot of adapter and BitFit usage in research. Vision models: fine-tuning a ConvNet vs. adding adapters – both have been tried; recently, techniques like LoRA have also been applied to diffusion models and vision Transformer ([Adapters](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter#:~:text=Low))】. Generally, as model size grows, the appeal of parameter-efficient methods grows too, because the cost of full fine-tuning scales linearly with *N*, but the cost of, say, a rank-8 LoRA scales roughly with O(N * r/ min(m,n)), which grows much slower than N if r is fixed. In summary, small models – you might just fine-tune fully; medium models – adapters/LoRA give efficiency with almost no loss; ultra-large models – adapters/LoRA/prefix *are required* and maintain surprisingly high performance if done righ ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=In%20Transformers%2C%20LoRa%20is%20typically,models%20up%20to%20175B%20parameters))】.

- **Performance (Accuracy) Trade-offs:** Full fine-tuning is often the gold standard for maximum performance. However, many studies show that **PEFT methods reach within a few points (or fractions of a point) of full fine-tuning** on standard benchmark ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=and%20new%20tasks%20can%20be,of%20the%20parameters%20per%20task)) ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))】. For example, Houlsby adapters were 0.4% off on GLU ([[1902.00751] Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751#:~:text=and%20new%20tasks%20can%20be,of%20the%20parameters%20per%20task))】; BitFit was on par for some tasks with small dat ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))】; LoRA often is <1% difference on metrics compared to full FT. There are cases where full fine-tuning has an edge – e.g., if a task requires a significant model reconfiguration that low-rank or small adapters can’t capture, or if the dataset is very large (millions of examples) such that updating all weights leads to a noticeably better solution. But for many typical tasks (with thousands to hundreds of thousands of training examples), the gap is minimal. One interesting observation: some parameter-efficient methods can sometimes *surpass* full fine-tuning on certain tasks, likely due to regularization effects. For instance, the BitFit paper reported slight improvements on a couple of GLUE tasks vs. full FT in low-data regime ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))】. Also, combining methods (e.g., fine-tuning a last layer + prompt tuning, or BitFit + LoRA) can sometimes outperform either alone, indicating these aren’t strictly exclusive choices. In tasks that require *multiple skills or domains*, methods like AdapterFusion can actually outperform a single fully fine-tuned model because they allow leveraging knowledge from multiple specialized adapters without interferenc ([[2005.00247] AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247#:~:text=%3E%20Abstract%3ASequential%20fine,classifier%20can%20effectively%20exploit%20the)) ([[2005.00247] AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://arxiv.org/abs/2005.00247#:~:text=empirically%20evaluate%20AdapterFusion%20on%2016,code%20and%20adapters%20are%20available))】. So, in multi-domain settings, using specialized adapters and then fusing them can beat a monolithic fine-tune that tries to memorize everything in one set of weights. In general, for **real-world applications**, the slight performance differences have to be weighed against practical constraints. If a fine-tuning method gets 99% of the accuracy for 1% of the computing cost, that’s usually a win.

- **Scalability and Multi-Task Use:** If you need to handle *many tasks or deployments*, parameter-efficient methods shine. Suppose you have a base model and want to serve 100 different classification tasks. Full fine-tuning would produce 100 models, each the size of the base – storage and memory blow up. With adapters or LoRA, you can instead keep one base model and 100 small modules. You load the base model once, and for each task just swap in the adapter or add the LoRA weights on the fly (some frameworks support fast switching of adapters). This is hugely advantageous in cloud deployment or on-device scenarios where memory is limited. Also, as mentioned, some approaches allow **combining tuners**: you could have an adapter for style and another for content and combine them if the methods are additive. Or in LoRA, people have shown you can train LoRA modules independently and then sum them to achieve combined effects (though not always trivial, it can work if the tasks are linear). For example, one might have a LoRA for polite tone and another for technical content and add them to get a polite + technical model (this is an active area of research but conceptually possible because LoRA updates are linear additions to weights). Full fine-tuning doesn’t offer this modularity; if you want a combined behavior, you have to fine-tune again on data that reflects both, or do multi-task training which is more complex.

- **Training Data Requirements:** Sometimes using fewer trainable parameters can be an advantage in low-resource settings. Because PEFT methods restrict the hypothesis space, they might require fewer examples to converge. For instance, if you only train biases, you’re fitting far fewer degrees of freedom, which might generalize better from small data. However, if data is extremely scarce, even those few parameters might not find a good signal, and one might prefer approaches like prompt tuning which effectively do zero-shot or few-shot adaptation by leveraging the model’s in-context abilities. On the flip side, if you have *massive* data for the task (comparable to pre-training data), full fine-tuning might benefit more from it since it can adjust all weights to absorb that data. In practice, most fine-tuning tasks have limited data compared to pretraining, so the ability of methods like LoRA to avoid overfitting while still training on enough capacity is valuable.

- **Catastrophic Forgetting and Reversibility:** Because full fine-tuning alters all weights, the original general-purpose model is lost (unless you save a copy). In contrast, methods that leave the base model untouched (adapters, LoRA, prefix, BitFit to an extent) inherently preserve the original model. This means if you remove the adapter or LoRA weights, you *get the original model back*. This is useful if you want to *temporarily specialize* a model and then use it for something else. For example, with LoRA, you can keep the LoRA weights separate and switch them on for task A and off for a general task, etc. This also means these methods protect the base model from forgetting its general abilities – only the small added parameters carry task-specific info. (However, note that if you fine-tune fully on one task, the model might degrade on others unless you do multi-task or continual learning strategies. PEFT avoids that by design.) From a product standpoint, this can be crucial: you might fine-tune a language model on proprietary data via LoRA and deploy it. If needed, you can always revert to the base model for another use-case, or apply a different LoRA for a different customer’s data, etc., without maintaining multiple full model instances.

- **Licensing and Model Sharing:** A subtle real-world consideration: often large pre-trained models come with licenses (e.g., Meta’s LLaMA is non-commercial). If you fine-tune the entire model and want to share it, you might be effectively sharing the base model weights (which could violate license). But sharing just an adapter or LoRA weights (which on their own don’t produce useful output without the base model) may be seen as a derived work that doesn’t fully reveal the original model. This has led to a practice in open-source of releasing LoRA diffs for models like LLaMA, which users can apply to the original (which they have to obtain themselves). It’s a gray area legally, but it’s another reason small, separable fine-tunes are convenient.

To summarize the comparative picture: **full fine-tuning** maximizes performance but at high compute/memory cost and with less flexibility. **Adapter-based and LoRA** methods achieve *nearly the same performance* in most cases, while reducing training size by orders of magnitude and enabling modular use – they are generally the sweet spot for large models and multi-task scenarios. **Prefix/prompt tuning** push efficiency to the extreme with minimal parameters and easy model sharing; they work best with very large models and are excellent when model weights cannot be touched (but may need large model size to reach equal performance). **BitFit** is a ultra-simple method that often works surprisingly well, though usually a bit less accurate than the above, it’s a nice option when one wants to fine-tune with practically no capacity overhead.

In practice, the choice often boils down to *requirements*: If you have a 100M parameter model and one task – go ahead and fine-tune it fully. If you have a 10B model and 5 tasks – using adapters or LoRA will save you a ton of hassle and hardware. If you have a 65B model and one GPU – QLoRA is basically your only option to fine-tune i ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is))】. If you have a proprietary model behind API – prompt tuning might be the only feasible method. Many modern pipelines actually combine approaches: e.g., use LoRA to fine-tune a large model and also do some prompt engineering on top for additional control. Or fine-tune a base model with adapters, and then at inference time still provide text prompts for specific user requests. These methods are not mutually exclusive and can complement each other.

## 5. Coding Examples in PyTorch

To illustrate how these fine-tuning methods are applied, we provide a few coding examples using PyTorch and the Hugging Face ecosystem. These examples assume some familiarity with PyTorch and Hugging Face Transformers. They are simplified for clarity (not optimized for speed or full training scripts) but demonstrate the core ideas:

### 5.1 Full Fine-Tuning a Pre-trained Model (Example)

In this example, we’ll fine-tune a BERT model for a text classification task (e.g., sentiment analysis). We use Hugging Face Transformers to load a pre-trained model and then train it on a labeled dataset. Full fine-tuning means all BERT weights will be updated.

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load a pre-trained BERT model and tokenizer for binary classification
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Example training data (normally, use a DataLoader for real dataset)
texts = ["I love this movie!", "This film was terrible..."]
labels = [1, 0]  # 1 = positive, 0 = negative
encodings = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
inputs = {k: v for k, v in encodings.items()}
inputs["labels"] = torch.tensor(labels)

# Set up optimizer for full fine-tuning (all parameters)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

model.train()
# Forward pass with loss computation
outputs = model(**inputs)
loss = outputs.loss
# Backward pass and parameter update
loss.backward()
optimizer.step()

print(f"Training loss: {loss.item():.4f}")
```

In a real scenario, you'd loop over batches of data and train for multiple epochs, but the snippet above shows the essential steps:

1. **Loading the model** – we use `AutoModelForSequenceClassification` with `num_labels=2` to get a BERT model with a classification head. By default, all parameters in this model are set `requires_grad=True`.
2. **Preparing data** – we tokenize a couple of example texts and get input tensors. The tokenizer handles converting text to token IDs and attention masks. We also prepare a tensor of labels.
3. **Optimizer** – we use AdamW with a low learning rate (2e-5 is common for BERT fine-tunin ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))】). All model parameters are passed to the optimizer.
4. **Training step** – we call `model(**inputs)`, which will return a `loss` because we provided `labels`. (The `AutoModelForSequenceClassification` is configured to compute loss if labels are given). We backpropagate with `loss.backward()` and update parameters with `optimizer.step()`.

After training (even one step in this toy example), the model's weights are updated. This means the BERT transformer layers have been slightly adjusted to better classify the given examples. Full fine-tuning is straightforward with the Transformers library – by default, all weights will be tuned. If we wanted to do **partial fine-tuning** (e.g., only fine-tune the last layer or last few layers), we could set `requires_grad=False` for the parameters we want frozen. For instance:

```python
for name, param in model.named_parameters():
    if name.startswith("bert.encoder.layer.0"):  # freeze the first encoder layer
        param.requires_grad = False
```

One could freeze more layers similarly. Hugging Face also offers the `model.base_model` attribute (for BERT, that’s the Transformer without the classification head) which can be used to freeze all transformer layers:

```python
for param in model.base_model.parameters():
    param.requires_grad = False
```

and then only the classification head (and perhaps LayerNorms) would train. This is the feature-based approach.

### 5.2 Fine-Tuning with LoRA using Hugging Face PEFT

Next, we demonstrate using **LoRA** for fine-tuning with the 🤗 **PEFT** library. We will load a model and then wrap it with `peft.get_peft_model` using a `LoraConfig`. This allows us to fine-tune only the low-rank adapters instead of all parameters.

First, ensure you have the `peft` library installed (`pip install peft`). Then:

```python
from transformers import AutoModelForSeq2SeqLM
from peft import LoraConfig, get_peft_model, TaskType

# Load a seq2seq model (for example, a large T5 model)
model_name = "t5-base"  # using a smaller model for demo; could be a large model
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Set up LoRA configuration: decide the rank, target modules, etc.
lora_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,   # the type of task - here a sequence-to-sequence language model
    r=8,                # LoRA rank
    lora_alpha=32,      # scaling factor
    lora_dropout=0.1,   # dropout on the LoRA weights
    target_modules=["q", "v"]  # which sub-modules to apply LoRA to (target names must match model's nn.Module names)
)

# Wrap the model with LoRA
lora_model = get_peft_model(model, lora_config)
lora_model.print_trainable_parameters()
```

When you run this, it will modify the `model` in-place to include LoRA layers on the specified target modules (in a T5, the query and value projection matrices might have names containing “q” and “v”). The `print_trainable_parameters()` will output something like: *“trainable params: X || all params: Y || trainable%: Z”*, confirming that only a small fraction are trainab ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=model%20%3D%20AutoModelForSeq2SeqLM,0.19151053100118282)) ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=model%20%3D%20get_peft_model%28model%2C%20peft_config%29%20model,0.19151053100118282))0】. For instance, you might see `trainable%: 0.2` or similar, meaning 0.2% of parameters will be updated.

Now, training proceeds as normal with this `lora_model`. All the frozen parameters won’t get updates, only the LoRA-added ones will. You can use the same `AdamW` optimizer on `lora_model.parameters()` – it internally knows only LoRA params require grad. After fine-tuning, if you want to use the model for inference, you have two choices:

- Use the `lora_model` as is, which will apply the LoRA adjustments on the fly.
- Or merge the LoRA weights into the base model for deployment (PEFT provides `lora_model.merge_and_unload()` to merge the trained LoRA weights into the original model, after which you can drop the PEFT wrappers).

Using LoRA is particularly useful for large models like `bloom-7b` or `flan-t5-xxl` where full fine-tuning would be very memory-heavy. The example above used `t5-base` for simplicity, but one could put a 3B or 11B model and still fine-tune on a single GPU with LoRA by greatly reducing gradient memory. The `target_modules` in the config should be set to the names of the linear layers you want to apply LoRA to. Common choices are the query and value projections in attention (often named like `self_attn.q_proj` in models, etc ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=UniPELT%20Mao%20et%C2%A0al,layer%20of%20the%20transformer%20block))5】, and sometimes the output projection. The Hugging Face documentation provides guidance on the exact module names for each model architectu ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=peft_config%20%3D%20LoraConfig%28%20task_type%3DTaskType,1))9】.

### 5.3 Adapter-Based Fine-Tuning Example

For adapter-based fine-tuning, we’ll use the **Adapter Transformers** integration. Hugging Face’s `transformers` library does not include adapters by default, but the `adapter-transformers` library extends it. We can add adapters to BERT and train them. (If not installed, do `pip install adapter-transformers`).

```python
from transformers import AutoAdapterModel, AdapterConfig

# Load a BERT model with adapter support
model = AutoAdapterModel.from_pretrained("bert-base-uncased")

# Add a new adapter
adapter_config = AdapterConfig.load("houlsby")  # load a pre-defined adapter architecture (Houlsby et al. style)
model.add_adapter("sst-2", config=adapter_config)
# Add a classification head for our task (SST-2 sentiment in GLUE, for example)
model.add_classification_head("sst-2", num_labels=2)

# Activate the adapter and head
model.train_adapter("sst-2")

# Now 'model' is ready to be trained with only the adapter and classification head weights unfrozen.
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, "is trainable.")
```

This code does the following:

- Uses `AutoAdapterModel` to get a BERT model that is adapter-aware.
- Adds an adapter named "sst-2" with a configuration. Here we load a preset "houlsby" config which inserts adapters after both attention and feed-forward sublayers (the original Houlsby 2019 design). Alternatively, "pfeiffer" config would insert one adapter after the feed-forward only (a slightly different strategy). You can also customize the adapter bottleneck size by passing arguments or using `AdapterConfig(mh_adapter=True, output_adapter=True, reduction_factor=16, ...)` manually.
- Adds a classification head for task "sst-2". Adapter Transformers library requires an explicit prediction head for classification/regression tasks.
- Calls `model.train_adapter("sst-2")`. This freezes all parameters except the ones related to the "sst-2" adapter and its classification he ([Quick Start — AdapterHub documentation](https://docs.adapterhub.ml/quickstart.html#:~:text=with%20adapters,sentiment%20of%20a%20given%20sentence)) ([Quick Start — AdapterHub documentation](https://docs.adapterhub.ml/quickstart.html#:~:text=Having%20loaded%20the%20model%2C%20we,class%20label%20for%20our%20sentence))3】. Essentially, all original BERT weights are set `requires_grad=False` internally, and the new adapter and head are `True`.
- We print which parameters are trainable to verify that only the adapter and head are (you would see names like "sst-2_adapter" and "sst-2_head" etc. as trainable).

Training this model would involve feeding inputs and labels and doing `optimizer.step()` similarly to the full fine-tune example, but now the number of trainable parameters is much smaller. The forward pass of the model will route the data through BERT and through the adapter layers, computing the classification output.

Under the hood, the adapter modules are implemented as small linear layers inside the Transformer blocks. Using `AdapterConfig.load("houlsby")` sets them up with a default reduction factor (like 16, meaning the adapter bottleneck is 1/16th the hidden size, etc.). You can adjust that if needed (a smaller reduction_factor means a larger adapter with more parameters).

The adapter approach can also be used in combination with full fine-tuning for the head: for example, you might freeze BERT and just train adapter+head, or even allow fine-tuning of the last Transformer layer plus adapters in earlier layers – the library is flexible. But the typical use is to freeze base, train adapters.

**Switching tasks/adapters:** After training, you could save the adapter via `model.save_adapter("./sst-2-adapter", "sst-2")`. You can load it back into a base model later and activate it. This way, you could train multiple adapters (say one for sentiment, one for topic, etc.) and switch between them by `model.set_active_adapters("topic")` or similar. AdapterFusion would be another step, where you’d train a fusion layer on top of multiple pre-trained adapters (this library supports that too).

### Additional Resources

For more detailed tutorials and resources on fine-tuning methods:

- The paper *“Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning”* by Lialin et al. (2023) provides an extensive survey of 40+ PEFT methods and a taxono ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=This%20paper%20presents%20a%20systematic,scale%20language%20models))1】, along with comparisons on efficiency and multi-billion parameter models.
- The Hugging Face blog and docs have guides on [adapter tuning](https://huggingface.co/docs/transformers/main/en/adapters), [LoRA](https://huggingface.co/docs/peft/en/conceptual_guides/adapter#low-rank-adaptation-lora), and [PEFT usage](https://huggingface.co/docs/peft) which show examples similar to above.
- Libraries like [AdapterHub](https://adapterhub.ml/) host a repository of pre-trained adapter modules for various tasks and languages that can be plugged into models.
- For prefix and prompt tuning, check out Lester et al.’s paper *“The Power of Scale for Parameter-Efficient Prompt Tuning ([The Power of Scale for Parameter-Efficient Prompt Tuning - ACL Anthology](https://aclanthology.org/2021.emnlp-main.243/#:~:text=any%20number%20of%20labeled%20examples,2021))3】 and Li & Liang’s *“Prefix-Tuning: Optimizing Continuous Prompts for Generation ([[2303.15647] Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://ar5iv.org/pdf/2303.15647.pdf#:~:text=7.2%20Prefix))1】 for the original explanations. There are also open-source implementations (e.g., in 🤗 PEFT under PromptTuningConfig and PrefixTuningConfig).
- The BitFit method is described in *“BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language Models”* by Ben-Zaken et al. (20 ([[2106.10199] BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199#:~:text=,specific%20linguistic))60】, which is a short and accessible read illustrating that even tuning <0.1% of parameters can work well.

Each method comes with trade-offs, and the best approach can depend on the specific use case constraints. By understanding and trying out these fine-tuning strategies, one can efficiently adapt large models to solve real-world problems with minimal cost and maximum reuse of pre-trained knowledg ([LLM Fine-tuning: Old school, new school, and everything in between - Fuzzy Labs](https://www.fuzzylabs.ai/blog-post/llm-fine-tuning-old-school-new-school-and-everything-in-between#:~:text=finetuning%20phase%20plays%20an%20important,for%20additional%20training)) ([GitHub - huggingface/peft: PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.](https://github.com/huggingface/peft#:~:text=State))78】

---

### **Low-Rank Adaptation (LoRA) Equations**

LoRA modifies the weight matrices in a neural network layer by adding a low-rank update. Given an original weight matrix of size , LoRA learns two much smaller matrices (size ) and (size ) such that the weight update is:

where , ensuring a significant reduction in the number of trainable parameters. The adapted weight matrix during training becomes:

This allows fine-tuning of only the small LoRA parameters and , keeping frozen.

---

### **BitFit (Bias-Only Fine-Tuning) Equation**

BitFit updates only the bias terms in a neural network layer, rather than the full weight matrices. If a standard layer operates as:

where is the weight matrix, is the input, and is the bias vector, then BitFit fine-tunes only , keeping frozen.

This effectively shifts neuron activation thresholds without modifying the learned feature transformations.

---

### **Prefix Tuning Equations**

Prefix tuning prepends learnable vectors to the key and value matrices in self-attention layers. If the attention mechanism computes:

then prefix tuning augments and by concatenating a prefix :

where and are trainable parameters optimized during fine-tuning.

---

### **Computational Cost Comparison**

The computational complexity for different fine-tuning methods can be estimated as follows:

- **Full Fine-Tuning:**
  
  where is the number of parameters in the model.

- **LoRA Fine-Tuning:**
  
  where is the low-rank size, significantly reducing the number of trainable parameters.

- **BitFit:**
  
  where is the number of bias terms, which is orders of magnitude smaller than .

- **Prefix/Prompt Tuning:**
  
  where is the number of learnable prompt tokens and is the model’s hidden size.

Each method trades off computational cost, efficiency, and accuracy depending on the task.

---

This should now properly display the equations in LaTeX-rendered form. Let me know if you need further refinements!